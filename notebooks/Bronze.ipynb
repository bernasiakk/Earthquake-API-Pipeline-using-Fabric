{"cells":[{"cell_type":"markdown","source":["### Get data from API and load it as .parquet file\n","(with 0 transformations)"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c6823d0b-f741-4fa4-8ede-cfd24b9ed6a5"},{"cell_type":"code","source":["import requests\n","import json\n","\n","url = f\"https://earthquake.usgs.gov/fdsnws/event/1/query?format=geojson&starttime={yesterday_date}&endtime={current_date}\"\n","\n","response = requests.get(url)\n","\n","if response.status_code == 200:\n","    # Get the JSON response\n","    data = response.json()\n","    data = data['features']\n","    \n","    # Specify the file name (and path if needed)\n","    file_path = f'/lakehouse/default/Files/{current_date}_earthquake_data.json'\n","    \n","    # Open the file in write mode ('w') and save the JSON data\n","    with open(file_path, 'w') as file:\n","        json.dump(data, file, indent=4)\n","        \n","    print(f\"Data successfully saved to {file_path}\")\n","else:\n","    print(\"Failed to fetch data. Status code:\", response.status_code)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"d4f19a47-e298-4a14-b5e5-b189e9276c9e","normalized_state":"finished","queued_time":"2024-10-18T03:59:29.2641144Z","session_start_time":null,"execution_start_time":"2024-10-18T03:59:29.7274195Z","execution_finish_time":"2024-10-18T03:59:30.6944501Z","parent_msg_id":"ffb5d4ae-6f11-402b-9e73-ef0835ab8087"},"text/plain":"StatementMeta(, d4f19a47-e298-4a14-b5e5-b189e9276c9e, 6, Finished, Available, Finished)"},"metadata":{}},{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/lakehouse/default/Files/Bronze/TODO_earthquake_data.json'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[17], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/lakehouse/default/Files/Bronze/TODO_earthquake_data.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Open the file in write mode ('w') and save the JSON data\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# The `json.dump` method serializes `data` as a JSON formatted stream to `file`\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# `indent=4` makes the file human-readable by adding whitespace\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(data, file, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData successfully saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m~/cluster-env/trident_env/lib/python3.11/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/lakehouse/default/Files/Bronze/TODO_earthquake_data.json'"]}],"execution_count":4,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f2192144-dc01-4a71-a059-a1855d3063cd"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default"},"dependencies":{"lakehouse":{"default_lakehouse":"66c75d15-7f2c-45b4-a01d-f711522b3390","known_lakehouses":[{"id":"66c75d15-7f2c-45b4-a01d-f711522b3390"}],"default_lakehouse_name":"earthquake_lakehouse","default_lakehouse_workspace_id":"c04a53b9-f373-4dc9-bf15-efe4b47a076b"}}},"nbformat":4,"nbformat_minor":5}